/**
 * Test Fluid Benchmarking - TypeScript Implementation
 * Demonstrates IRT-based evaluation with mislabel detection
 */

import { FluidBenchmarking, createDefaultTestDataset, type IRTItem } from './frontend/lib/fluid-benchmarking';

async function testFluidBenchmarkingTS() {
  console.log('\nüî¨ FLUID BENCHMARKING - TypeScript Implementation Test\n');
  console.log('=' .repeat(80));
  
  // Create test dataset
  const testDataset = createDefaultTestDataset();
  console.log(`\nüìä Test Dataset: ${testDataset.length} items`);
  console.log(`   Easy (b < -0.5): ${testDataset.filter(i => i.difficulty < -0.5).length}`);
  console.log(`   Medium (-0.5 to 0.5): ${testDataset.filter(i => i.difficulty >= -0.5 && i.difficulty <= 0.5).length}`);
  console.log(`   Hard (b > 0.5): ${testDataset.filter(i => i.difficulty > 0.5).length}`);
  
  // Initialize evaluator
  const evaluator = new FluidBenchmarking(testDataset);
  
  // Mock test function (simulates Knowledge Graph with ability ~0.6)
  const mockKGTest = async (item: IRTItem): Promise<boolean> => {
    const kg_ability = 0.6;
    const p_correct = evaluator.probabilityCorrect(
      kg_ability,
      item.difficulty,
      item.discrimination
    );
    // Simulate probabilistic success
    return Math.random() < p_correct;
  };
  
  console.log('\n' + '=' .repeat(80));
  console.log('\nüìä Simulating Knowledge Graph Evaluation (ability ‚âà 0.6)...\n');
  
  // Run fluid benchmarking
  const result = await evaluator.fluidBenchmarking(
    'knowledge_graph',
    mockKGTest,
    {
      start_ability: 0.0,
      n_max: 30,
      estimation_method: 'map',
      early_stop_threshold: 0.3
    }
  );
  
  console.log('\n‚úÖ EVALUATION COMPLETE\n');
  console.log('=' .repeat(80));
  
  console.log('\nüìà Results:');
  console.log(`   Method: ${result.method}`);
  console.log(`   Final Ability: Œ∏ = ${result.final_ability.toFixed(3)}`);
  console.log(`   Standard Error: ¬±${result.standard_error.toFixed(3)}`);
  console.log(`   95% CI: [${result.ability_estimate.confidence_interval_95[0].toFixed(2)}, ${result.ability_estimate.confidence_interval_95[1].toFixed(2)}]`);
  console.log(`   Items Administered: ${result.items_administered}`);
  console.log(`   Items Correct: ${result.items_correct}`);
  console.log(`   Raw Accuracy: ${(result.accuracy * 100).toFixed(1)}%`);
  console.log(`   Processing Time: ${result.processing_time_ms}ms`);
  
  // Interpret ability
  console.log('\nüéØ Interpretation:');
  console.log(`   Ability Level: ${evaluator.interpretAbility(result.final_ability)}`);
  
  // Expected performance by difficulty
  const easyAccuracy = evaluator.expectedAccuracy(result.final_ability, [-2.0, -0.5]);
  const mediumAccuracy = evaluator.expectedAccuracy(result.final_ability, [-0.5, 0.5]);
  const hardAccuracy = evaluator.expectedAccuracy(result.final_ability, [0.5, 2.0]);
  
  console.log('\n   Expected Accuracy by Difficulty:');
  console.log(`   ‚Ä¢ Easy items (b < -0.5): ${(easyAccuracy * 100).toFixed(1)}%`);
  console.log(`   ‚Ä¢ Medium items (-0.5 to 0.5): ${(mediumAccuracy * 100).toFixed(1)}%`);
  console.log(`   ‚Ä¢ Hard items (b > 0.5): ${(hardAccuracy * 100).toFixed(1)}%`);
  
  // Simulate second method for comparison
  console.log('\n' + '=' .repeat(80));
  console.log('\nüìä Simulating LangStruct Evaluation (ability ‚âà 1.4)...\n');
  
  const mockLSTest = async (item: IRTItem): Promise<boolean> => {
    const ls_ability = 1.4;
    const p_correct = evaluator.probabilityCorrect(
      ls_ability,
      item.difficulty,
      item.discrimination
    );
    return Math.random() < p_correct;
  };
  
  const lsResult = await evaluator.fluidBenchmarking(
    'langstruct',
    mockLSTest,
    {
      start_ability: 0.0,
      n_max: 30,
      estimation_method: 'map'
    }
  );
  
  console.log('\n‚úÖ EVALUATION COMPLETE\n');
  console.log('=' .repeat(80));
  
  console.log('\nüìà Results:');
  console.log(`   Method: ${lsResult.method}`);
  console.log(`   Final Ability: Œ∏ = ${lsResult.final_ability.toFixed(3)}`);
  console.log(`   Standard Error: ¬±${lsResult.standard_error.toFixed(3)}`);
  console.log(`   95% CI: [${lsResult.ability_estimate.confidence_interval_95[0].toFixed(2)}, ${lsResult.ability_estimate.confidence_interval_95[1].toFixed(2)}]`);
  console.log(`   Items Administered: ${lsResult.items_administered}`);
  console.log(`   Items Correct: ${lsResult.items_correct}`);
  console.log(`   Raw Accuracy: ${(lsResult.accuracy * 100).toFixed(1)}%`);
  
  // Compare methods
  console.log('\n' + '=' .repeat(80));
  console.log('\nüèÜ COMPARISON\n');
  
  const abilityDiff = lsResult.final_ability - result.final_ability;
  const seCombined = Math.sqrt(
    Math.pow(result.standard_error, 2) + Math.pow(lsResult.standard_error, 2)
  );
  const zScore = abilityDiff / seCombined;
  const isSignificant = Math.abs(zScore) > 1.96; // p < 0.05
  
  console.log(`   Knowledge Graph: Œ∏ = ${result.final_ability.toFixed(2)} ¬± ${result.standard_error.toFixed(2)}`);
  console.log(`   LangStruct: Œ∏ = ${lsResult.final_ability.toFixed(2)} ¬± ${lsResult.standard_error.toFixed(2)}`);
  console.log(`\n   Difference: ${abilityDiff.toFixed(2)}`);
  console.log(`   Z-score: ${zScore.toFixed(2)}`);
  console.log(`   Statistical Significance: ${isSignificant ? '‚úÖ YES (p < 0.05)' : '‚ùå NO'}`);
  
  if (abilityDiff > 0.5) {
    console.log(`\n   üí° Recommendation: LangStruct significantly better for complex extractions`);
  } else if (abilityDiff < -0.5) {
    console.log(`\n   üí° Recommendation: Knowledge Graph surprisingly effective!`);
  } else {
    console.log(`\n   üí° Recommendation: Methods have similar performance`);
  }
  
  // Detect mislabeled items
  console.log('\n' + '=' .repeat(80));
  console.log('\nüîç MISLABEL DETECTION\n');
  
  const mislabeled = evaluator.identifyMislabeledItems(0.3);
  
  if (mislabeled.length > 0) {
    console.log(`   Found ${mislabeled.length} potentially mislabeled items:\n`);
    mislabeled.slice(0, 3).forEach(item => {
      console.log(`   ‚Ä¢ ${item.id}:`);
      console.log(`     Text: "${item.text.substring(0, 60)}..."`);
      console.log(`     Mislabel Probability: ${(item.mislabeled_probability! * 100).toFixed(0)}%`);
      console.log(`     Recommendation: Review and relabel\n`);
    });
  } else {
    console.log(`   ‚úÖ No mislabeled items detected (threshold: 30%)`);
  }
  
  console.log('=' .repeat(80));
  console.log('\n‚úÖ TypeScript Fluid Benchmarking Test Complete!\n');
  
  // Summary
  console.log('üìä SUMMARY:\n');
  console.log(`   ‚Ä¢ IRT-based ability estimation: ‚úÖ`);
  console.log(`   ‚Ä¢ Adaptive item selection: ‚úÖ`);
  console.log(`   ‚Ä¢ Confidence intervals: ‚úÖ`);
  console.log(`   ‚Ä¢ Mislabel detection: ‚úÖ`);
  console.log(`   ‚Ä¢ Statistical comparison: ‚úÖ`);
  console.log(`   ‚Ä¢ All in TypeScript: ‚úÖ\n`);
  
  return { result, lsResult, mislabeled };
}

// Run test
testFluidBenchmarkingTS()
  .then(() => process.exit(0))
  .catch(error => {
    console.error('\n‚ùå Test failed:', error);
    process.exit(1);
  });

