/**
 * MCMC-Inspired Inference Sampling
 *
 * Based on "Reasoning with Sampling: Your Base Model is Smarter Than You Think"
 * arXiv:2510.14901
 *
 * Key Innovation: Enhances base LLM reasoning at inference time without training
 * by sampling from sharpened distributions using the model's own likelihoods.
 *
 * Benefits:
 * - No training required (drop-in enhancement)
 * - Maintains diversity (vs RL collapse)
 * - Performance boost (matches RL without overhead)
 * - Broad applicability (works with any model)
 *
 * Usage:
 * ```typescript
 * const samples = await mcmcSampling({
 *   model: 'gpt-4o-mini',
 *   prompt: 'Reformulate this query: "Q4 revenue?"',
 *   numSamples: 10,
 *   beta: 1.5,
 *   topK: 5
 * });
 * // Returns: 5 diverse, high-quality reformulations
 * ```
 */

export interface SamplingConfig {
  model: string;           // 'gpt-4o-mini', 'ollama/llama3.1:8b', 'claude-3-5-sonnet', etc.
  prompt: string;          // Prompt to generate from
  numSamples: number;      // Total samples to generate
  beta: number;            // Sharpening factor (1.0 = no sharpening, 2.0 = strong quality bias)
  temperature?: number;    // Base sampling temperature (default: 1.0)
  topK?: number;           // Return best k samples (default: numSamples)
  maxTokens?: number;      // Max tokens per sample (default: 1024)
}

export interface SamplingResult {
  samples: string[];       // Selected samples (length = topK)
  likelihoods: number[];   // Log likelihoods for each sample
  diversity: number;       // Diversity score (0-1)
  avgQuality: number;      // Average quality score (0-1)
  metadata: {
    totalGenerated: number;
    accepted: number;
    rejected: number;
    modelUsed: string;
  };
}

interface SampleCandidate {
  text: string;
  logProb: number;
  sharpenedProb: number;
  accepted: boolean;
}

/**
 * Main MCMC sampling function
 *
 * Generates diverse, high-quality samples by:
 * 1. Sampling from base model
 * 2. Calculating log likelihoods
 * 3. Sharpening distribution (beta parameter)
 * 4. Metropolis-Hastings acceptance
 * 5. Diversity-aware top-k selection
 */
export async function mcmcSampling(config: SamplingConfig): Promise<SamplingResult> {
  const {
    model,
    prompt,
    numSamples,
    beta,
    temperature = 1.0,
    topK = numSamples,
    maxTokens = 1024
  } = config;

  console.log(`ðŸŽ² MCMC Sampling: model=${model}, samples=${numSamples}, beta=${beta}, topK=${topK}`);

  const startTime = Date.now();
  const candidates: SampleCandidate[] = [];

  // Generate samples with Metropolis-Hastings acceptance
  for (let i = 0; i < numSamples; i++) {
    try {
      // Generate sample from model
      const text = await generateSample(model, prompt, temperature, maxTokens);

      // Calculate log likelihood
      const logProb = await calculateLogLikelihood(model, prompt, text);

      // Sharpen distribution
      const sharpenedProb = logProb * beta;

      // Metropolis-Hastings acceptance
      const accepted = acceptSample(sharpenedProb);

      candidates.push({
        text,
        logProb,
        sharpenedProb,
        accepted
      });

      if (accepted) {
        console.log(`  âœ… Sample ${i + 1}/${numSamples} accepted (logProb: ${logProb.toFixed(3)})`);
      } else {
        console.log(`  âŒ Sample ${i + 1}/${numSamples} rejected (logProb: ${logProb.toFixed(3)})`);
      }
    } catch (error) {
      console.error(`  âš ï¸ Sample ${i + 1}/${numSamples} failed:`, error);
      continue;
    }
  }

  // Filter accepted samples
  const acceptedCandidates = candidates.filter(c => c.accepted);

  if (acceptedCandidates.length === 0) {
    console.warn('âš ï¸ No samples accepted! Falling back to all candidates.');
    acceptedCandidates.push(...candidates);
  }

  // Select top-k by diversity + quality
  const selected = selectTopK(acceptedCandidates, topK);

  const latency = Date.now() - startTime;

  console.log(`âœ… MCMC Sampling complete: ${selected.length} samples selected in ${latency}ms`);
  console.log(`   Diversity: ${calculateDiversity(selected).toFixed(3)}`);
  console.log(`   Avg Quality: ${(selected.reduce((sum, s) => sum + s.sharpenedProb, 0) / selected.length).toFixed(3)}`);

  return {
    samples: selected.map(s => s.text),
    likelihoods: selected.map(s => s.logProb),
    diversity: calculateDiversity(selected),
    avgQuality: selected.reduce((sum, s) => sum + s.sharpenedProb, 0) / selected.length,
    metadata: {
      totalGenerated: candidates.length,
      accepted: acceptedCandidates.length,
      rejected: candidates.length - acceptedCandidates.length,
      modelUsed: model
    }
  };
}

/**
 * Generate single sample from model
 */
async function generateSample(
  model: string,
  prompt: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  // Route to appropriate model
  if (model.startsWith('ollama/')) {
    return await generateOllama(model, prompt, temperature, maxTokens);
  } else if (model.startsWith('perplexity/') || model.startsWith('pplx-')) {
    return await generatePerplexity(model, prompt, temperature, maxTokens);
  } else if (model.startsWith('gpt-') || model.startsWith('o1-')) {
    // Route all OpenAI-prefixed models to Perplexity sonar-pro (no OpenAI usage)
    const fallback = 'sonar-pro';
    return await generatePerplexity(fallback, prompt, temperature, maxTokens);
  } else if (model.startsWith('claude-')) {
    return await generateAnthropic(model, prompt, temperature, maxTokens);
  }

  throw new Error(`Unknown model: ${model}`);
}

/**
 * Calculate log likelihood of response under model
 *
 * Uses model's logprobs API when available, otherwise approximates
 * based on response length (shorter = higher probability).
 */
async function calculateLogLikelihood(
  model: string,
  prompt: string,
  response: string
): Promise<number> {
  // For OpenAI models with logprobs API
  if (model.startsWith('gpt-') && !model.startsWith('gpt-4o')) {
    try {
      const result = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: model,
          messages: [
            { role: 'user', content: prompt },
            { role: 'assistant', content: response }
          ],
          logprobs: true,
          max_tokens: 1
        })
      });

      if (result.ok) {
        const data = await result.json();
        const logprobs = data.choices[0]?.logprobs?.content || [];
        return logprobs.reduce((sum: number, token: any) => sum + (token.logprob || 0), 0);
      }
    } catch (error) {
      console.warn('Failed to get logprobs, using fallback');
    }
  }

  // Fallback: approximate based on response characteristics
  return calculateApproximateLogLikelihood(prompt, response);
}

/**
 * Approximate log likelihood based on response characteristics
 *
 * Heuristics:
 * - Shorter responses: higher probability (more confident)
 * - Better grammar: higher probability
 * - Relevance to prompt: higher probability
 */
function calculateApproximateLogLikelihood(prompt: string, response: string): number {
  let logProb = 0;

  // Length penalty (prefer concise responses)
  const tokens = response.split(/\s+/).length;
  logProb -= tokens / 100;

  // Relevance bonus (check overlap with prompt)
  const promptWords = new Set(prompt.toLowerCase().split(/\s+/));
  const responseWords = new Set(response.toLowerCase().split(/\s+/));
  const overlap = new Set([...promptWords].filter(w => responseWords.has(w)));
  const relevance = overlap.size / Math.max(promptWords.size, 1);
  logProb += relevance * 2;

  // Completeness bonus (has proper ending)
  if (response.match(/[.!?]$/)) {
    logProb += 0.5;
  }

  // Quality bonus (no placeholder text)
  if (!response.match(/\[.*\]|TODO|FIXME|placeholder/i)) {
    logProb += 0.5;
  }

  return logProb;
}

/**
 * Metropolis-Hastings acceptance criterion
 *
 * Accept sample with probability exp(sharpenedProb).
 * This ensures we sample from the sharpened distribution.
 */
function acceptSample(sharpenedProb: number): boolean {
  // Normalize to probability (0-1 range)
  const acceptanceProb = 1 / (1 + Math.exp(-sharpenedProb));

  // Accept with this probability
  return Math.random() < acceptanceProb;
}

/**
 * Select top-k samples balancing quality and diversity
 *
 * Algorithm:
 * 1. Sort by sharpened probability (quality)
 * 2. Greedily select diverse samples (avoid redundancy)
 */
function selectTopK(
  candidates: SampleCandidate[],
  k: number
): SampleCandidate[] {
  if (candidates.length <= k) {
    return candidates;
  }

  // Sort by quality (sharpened probability)
  const sorted = [...candidates].sort((a, b) => b.sharpenedProb - a.sharpenedProb);

  // Greedy diversity selection
  const selected: SampleCandidate[] = [sorted[0]]; // Always take best

  for (const candidate of sorted.slice(1)) {
    if (selected.length >= k) break;

    // Calculate diversity from selected set
    const minDistance = Math.min(
      ...selected.map(s => semanticDistance(s.text, candidate.text))
    );

    // Accept if diverse enough (distance > threshold)
    if (minDistance > 0.3) {
      selected.push(candidate);
    }
  }

  // If we didn't get enough diverse samples, fill with highest quality
  if (selected.length < k) {
    for (const candidate of sorted) {
      if (selected.length >= k) break;
      if (!selected.includes(candidate)) {
        selected.push(candidate);
      }
    }
  }

  return selected;
}

/**
 * Calculate semantic distance between two texts
 *
 * Uses Jaccard distance on tokenized text:
 * distance = 1 - (intersection / union)
 */
function semanticDistance(text1: string, text2: string): number {
  const tokens1 = new Set(text1.toLowerCase().split(/\s+/));
  const tokens2 = new Set(text2.toLowerCase().split(/\s+/));

  const intersection = new Set([...tokens1].filter(t => tokens2.has(t)));
  const union = new Set([...tokens1, ...tokens2]);

  const jaccard = intersection.size / union.size;
  return 1 - jaccard; // Distance = 1 - similarity
}

/**
 * Calculate diversity score for sample set
 *
 * Returns average pairwise distance (0-1 scale).
 * Higher = more diverse.
 */
function calculateDiversity(samples: SampleCandidate[]): number {
  if (samples.length <= 1) return 0;

  let totalDistance = 0;
  let comparisons = 0;

  for (let i = 0; i < samples.length; i++) {
    for (let j = i + 1; j < samples.length; j++) {
      totalDistance += semanticDistance(samples[i].text, samples[j].text);
      comparisons++;
    }
  }

  return totalDistance / comparisons;
}

// ============================================================================
// Model-Specific Generation Functions
// ============================================================================

/**
 * Generate sample from Ollama (local model)
 */
async function generateOllama(
  model: string,
  prompt: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  const modelName = model.replace('ollama/', '');
  const ollamaHost = process.env.OLLAMA_HOST || 'http://localhost:11434';

  const response = await fetch(`${ollamaHost}/api/generate`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      model: modelName,
      prompt: prompt,
      temperature: temperature,
      stream: false,
      options: {
        num_predict: maxTokens
      }
    })
  });

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.response;
}

/**
 * Generate sample from OpenAI
 */
async function generateOpenAI(
  model: string,
  prompt: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      temperature: temperature,
      max_tokens: maxTokens
    })
  });

  if (!response.ok) {
    throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.choices[0].message.content;
}

/**
 * Generate sample from Anthropic (Claude)
 */
async function generateAnthropic(
  model: string,
  prompt: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'x-api-key': process.env.ANTHROPIC_API_KEY!,
      'anthropic-version': '2023-06-01',
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      temperature: temperature,
      max_tokens: maxTokens
    })
  });

  if (!response.ok) {
    throw new Error(`Anthropic API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.content[0].text;
}

/**
 * Generate sample from Perplexity
 */
async function generatePerplexity(
  model: string,
  prompt: string,
  temperature: number,
  maxTokens: number
): Promise<string> {
  const pplxModel = model.startsWith('perplexity/') ? model.replace('perplexity/', '') : model; // e.g., pplx-70b-online
  const response = await fetch('https://api.perplexity.ai/chat/completions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${process.env.PERPLEXITY_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      model: pplxModel,
      messages: [{ role: 'user', content: prompt }],
      temperature,
      max_tokens: maxTokens
    })
  });

  if (!response.ok) {
    throw new Error(`Perplexity API error: ${response.status} ${response.statusText}`);
  }

  const data = await response.json();
  return data.choices?.[0]?.message?.content || '';
}

// ============================================================================
// Helper Functions
// ============================================================================

/**
 * Quick helper: Generate diverse samples with sensible defaults
 */
export async function generateDiverseSamples(
  model: string,
  prompt: string,
  count: number = 5
): Promise<string[]> {
  const result = await mcmcSampling({
    model,
    prompt,
    numSamples: count * 2, // Oversample for diversity
    beta: 1.5,             // Moderate sharpening
    topK: count
  });

  return result.samples;
}

/**
 * Quick helper: Generate high-quality samples (less diversity, more quality)
 */
export async function generateQualitySamples(
  model: string,
  prompt: string,
  count: number = 3
): Promise<string[]> {
  const result = await mcmcSampling({
    model,
    prompt,
    numSamples: count * 2,
    beta: 2.0,  // Strong quality bias
    topK: count
  });

  return result.samples;
}

/**
 * Quick helper: Single best sample (maximum quality)
 */
export async function generateBestSample(
  model: string,
  prompt: string
): Promise<string> {
  const result = await mcmcSampling({
    model,
    prompt,
    numSamples: 5,
    beta: 2.5,  // Very strong quality bias
    topK: 1
  });

  return result.samples[0];
}
