import { NextRequest, NextResponse } from 'next/server';
import QualityFirstTrainingSystem from '../../../../lib/quality-first-training-system';

const qualityFirstTrainingSystem = new QualityFirstTrainingSystem();

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { 
      operation, 
      targetSize,
      targetLength,
      varianceThreshold,
      candidateProblems 
    } = body;

    console.log('üéØ Quality-First Training System API:', { 
      operation, 
      targetSize,
      targetLength 
    });

    let result;

    switch (operation) {
      case 'quality-first-pipeline':
        result = await qualityFirstTrainingSystem.executeQualityFirstTraining();
        break;

      case 'mine-graduate-questions':
        const graduateQuestions = await qualityFirstTrainingSystem.naturalReasoningDatasetAccess.mineGraduateLevelQuestions();
        result = {
          graduateQuestions,
          methodology: ['NaturalReasoning Dataset: Web-grounded, graduate-level questions'],
          breakthrough: '2.8M questions mined from pre-training corpora across major scientific fields'
        };
        break;

      case 'generate-chain-of-thought':
        const chainOfThoughtQuestions = await qualityFirstTrainingSystem.naturalReasoningDatasetAccess.generateChainOfThoughtQuestions(
          targetLength || 434
        );
        result = {
          chainOfThoughtQuestions,
          methodology: ['Chain-of-Thought Optimization: Longest median chain-of-thought (434 words)'],
          breakthrough: 'Elicit the longest median chain-of-thought among open datasets'
        };
        break;

      case 'evaluate-dataset-quality':
        const qualityMetrics = await qualityFirstTrainingSystem.naturalReasoningDatasetAccess.evaluateDatasetQuality();
        result = {
          qualityMetrics,
          methodology: ['Dataset Quality Evaluation: Comprehensive quality assessment'],
          breakthrough: 'Quality and diversity metrics for training optimization'
        };
        break;

      case 'distill-llama-8b':
        const graduateQuestionsForDistillation = await qualityFirstTrainingSystem.naturalReasoningDatasetAccess.mineGraduateLevelQuestions();
        const distillationResult = await qualityFirstTrainingSystem.dataDistillationSystemAccess.distillLlama8B(
          graduateQuestionsForDistillation, 
          targetSize || 1000000
        );
        result = {
          distillationResult,
          methodology: ['Data Distillation: 8B Llama on 0.5-2M NR problems vs larger datasets'],
          breakthrough: 'Steeper accuracy gains than training on larger WebInstruct/OpenMathInstruct sets'
        };
        break;

      case 'lilo-optimal-selection':
        const liloResult = await qualityFirstTrainingSystem.liloMethodAccess.identifyOptimalProblems(
          candidateProblems || [],
          targetSize || 1000
        );
        result = {
          liloResult,
          methodology: ['LILO Method: Automatic selection of optimal training problems'],
          breakthrough: 'Algorithmically identify questions that allow for maximally efficient training'
        };
        break;

      case 'prioritize-high-variance':
        const problems = candidateProblems?.map((p: string, i: number) => ({
          id: `problem_${i}`,
          problem: p,
          learnability: Math.random() * 0.5 + 0.5,
          variance: Math.random() * 0.5 + 0.3,
          diversity: Math.random() * 0.4 + 0.6,
          efficiency: Math.random() * 0.3 + 0.6,
          liloScore: Math.random() * 0.4 + 0.6,
          selected: false
        })) || [];
        const highVarianceProblems = await qualityFirstTrainingSystem.liloMethodAccess.prioritizeHighVarianceQuestions(
          problems,
          varianceThreshold || 0.7
        );
        result = {
          highVarianceProblems,
          methodology: ['Prioritize High Variance Questions: Questions with high learnability'],
          breakthrough: 'High variance of success questions for efficient training'
        };
        break;

      case 'demonstrate-efficiency-gains':
        const selectedProblems = candidateProblems?.map((p: string, i: number) => ({
          id: `selected_${i}`,
          problem: p,
          learnability: Math.random() * 0.5 + 0.5,
          variance: Math.random() * 0.5 + 0.3,
          diversity: Math.random() * 0.4 + 0.6,
          efficiency: Math.random() * 0.3 + 0.6,
          liloScore: Math.random() * 0.4 + 0.6,
          selected: true
        })) || [];
        const efficiencyGains = await qualityFirstTrainingSystem.liloMethodAccess.demonstrateEfficiencyGains(
          selectedProblems,
          []
        );
        result = {
          efficiencyGains,
          methodology: ['Demonstrate Efficiency Gains: 3x fewer training steps with higher accuracy'],
          breakthrough: 'Achieve higher final test accuracy in 3x fewer training steps'
        };
        break;

      default:
        return NextResponse.json({
          success: false,
          error: 'Invalid operation. Supported operations: quality-first-pipeline, mine-graduate-questions, generate-chain-of-thought, evaluate-dataset-quality, distill-llama-8b, lilo-optimal-selection, prioritize-high-variance, demonstrate-efficiency-gains'
        }, { status: 400 });
    }

    console.log('‚úÖ Quality-First Training operation completed:', { 
      operation, 
      resultKeys: Object.keys(result || {}) 
    });

    return NextResponse.json({
      success: true,
      operation,
      result,
      timestamp: new Date().toISOString(),
      metadata: {
        processingTime: Date.now(),
        components: [
          'Natural Reasoning Dataset',
          'Data Distillation System',
          'LILO Method',
          'Quality-First Training Pipeline'
        ]
      }
    });

  } catch (error) {
    console.error('‚ùå Quality-First Training System API error:', error);
    
    return NextResponse.json({
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error occurred',
      timestamp: new Date().toISOString()
    }, { status: 500 });
  }
}

export async function GET() {
  return NextResponse.json({
    success: true,
    message: 'Quality-First Training System API - Prioritizing Quality and Diversity Over Volume',
    paradigmShift: {
      from: 'Volume-based training',
      to: 'Quality and diversity-based training',
      innovation: 'NaturalReasoning dataset with graduate-level questions',
      performance: 'Steeper accuracy gains with smaller, higher-quality datasets',
      efficiency: '3x fewer training steps with higher final test accuracy'
    },
    availableOperations: [
      {
        operation: 'quality-first-pipeline',
        description: 'Complete quality-first training pipeline with all components',
        parameters: []
      },
      {
        operation: 'mine-graduate-questions',
        description: 'Mine graduate-level questions from pre-training corpora',
        parameters: []
      },
      {
        operation: 'generate-chain-of-thought',
        description: 'Generate chain-of-thought optimized questions',
        parameters: ['targetLength']
      },
      {
        operation: 'evaluate-dataset-quality',
        description: 'Evaluate dataset quality metrics',
        parameters: []
      },
      {
        operation: 'distill-llama-8b',
        description: 'Distill 8B Llama on Natural Reasoning problems',
        parameters: ['targetSize']
      },
      {
        operation: 'lilo-optimal-selection',
        description: 'LILO method for optimal training problem selection',
        parameters: ['candidateProblems', 'targetSize']
      },
      {
        operation: 'prioritize-high-variance',
        description: 'Prioritize high variance of success questions',
        parameters: ['candidateProblems', 'varianceThreshold']
      },
      {
        operation: 'demonstrate-efficiency-gains',
        description: 'Demonstrate efficiency gains with learnability-based training',
        parameters: ['candidateProblems']
      }
    ],
    qualityFirstParadigms: {
      'natural-reasoning-dataset': {
        description: 'Web-grounded, graduate-level questions for mathematical & scientific reasoning',
        innovation: '2.8M questions mined from pre-training corpora across major scientific fields',
        performance: 'Longest median chain-of-thought (434 words) among open datasets'
      },
      'data-distillation': {
        description: 'Distill 8B Llama on 0.5-2M NR problems vs larger datasets',
        innovation: 'Steeper accuracy gains than training on larger WebInstruct/OpenMathInstruct sets',
        efficiency: 'Cut tokens and compute while maintaining performance'
      },
      'lilo-method': {
        description: 'Automatic selection of optimal training problems',
        innovation: 'Algorithmically identify questions that allow for maximally efficient training',
        performance: 'Prioritize training on questions with high variance of success'
      },
      'learnability-based-training': {
        description: 'High variance of success questions for efficient training',
        innovation: 'Questions with high learnability allow for maximally efficient training',
        efficiency: 'Achieve higher final test accuracy in 3x fewer training steps'
      }
    },
    capabilities: [
      'Natural Reasoning Dataset: Web-grounded, graduate-level questions',
      'Chain-of-Thought Optimization: Longest median chain-of-thought (434 words)',
      'Data Distillation: 8B Llama on 0.5-2M NR problems vs larger datasets',
      'LILO Method: Automatic selection of optimal training problems',
      'Learnability-Based Training: High variance of success questions',
      'Efficiency Optimization: 3x fewer training steps with higher accuracy',
      'Quality Metrics: Comprehensive dataset quality assessment',
      'Performance Comparison: Superior results with smaller, higher-quality datasets'
    ],
    researchInsights: [
      'Researchers prioritize quality and diversity of post-training data over volume',
      'NaturalReasoning dataset uses web-grounded, graduate-level questions',
      '2.8M questions mined from pre-training corpora across major scientific fields',
      'Longest median chain-of-thought (434 words) among open datasets',
      'Distilling 8B Llama on 0.5-2M NR problems yields steeper accuracy gains',
      'LILO method algorithmically identifies optimal training problems',
      'High variance of success questions enable maximally efficient training',
      '3x fewer training steps with higher final test accuracy'
    ],
    technicalBreakthrough: {
      paradigm: 'Quality and diversity over volume in post-training data',
      dataset: 'NaturalReasoning: 2.8M graduate-level questions',
      chainOfThought: '434 words median chain-of-thought length',
      distillation: '8B Llama on 0.5-2M NR problems outperforms larger datasets',
      lilo: 'Automatic optimal training problem selection',
      learnability: 'High variance of success questions for efficient training',
      efficiency: '3x fewer training steps with higher final test accuracy',
      innovation: 'Prioritizing training on questions with high variance of success'
    }
  });
}
