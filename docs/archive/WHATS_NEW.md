# ğŸ‰ What's New: Enterprise Memory System

## ğŸš€ Just Added

We've built a complete **Hyperspell-like memory system** integrated with your existing GEPA/AX stack. Here's what's new:

---

## âœ… 1. Vector Search & Indexed Memories

**What it does:** Store and search your data using semantic similarity (not just keywords)

**Key Features:**
- ğŸ” Vector embeddings using OpenAI `text-embedding-3-small`
- ğŸ’¾ PostgreSQL + pgvector for blazing-fast similarity search
- ğŸ“Š 1536-dimension vectors with IVFFLAT indexing
- ğŸ¯ Threshold-based relevance filtering

**API:** `/api/search/indexed`

**Example:**
```typescript
const results = await memoryClient.indexedSearch(
  "machine learning algorithms",
  user.id,
  { matchThreshold: 0.8, matchCount: 10 }
);
```

---

## âœ… 2. Multi-Model Answer Routing

**What it does:** Automatically picks the best AI model based on your query type

**Smart Routing:**
- ğŸ§® **Math/Code** â†’ `o1-mini` (reasoning specialist)
- ğŸ’» **Code** â†’ `gpt-4o` (programming expert)
- ğŸ”¬ **Scientific** â†’ `claude-3-sonnet` (deep reasoning)
- âš¡ **General** â†’ `claude-3-haiku` (fast responses)

**API:** `/api/answer`

**Example:**
```typescript
const answer = await generateAnswer(
  "solve: 2x + 5 = 15",
  documents,
  { autoSelectModel: true }
);
// Automatically uses o1-mini for math!
```

---

## âœ… 3. Multi-Source Search with Error Handling

**What it does:** Search multiple data sources simultaneously, gracefully handle failures

**Sources:**
- ğŸ“š **Vault** - Your personal indexed memories
- ğŸŒ **Web** - Live search via Perplexity
- ğŸ“„ **Documents** - Uploaded files
- ğŸ” **Indexed** - Vector similarity search

**Error Handling:**
- âœ… Returns partial results if some sources fail
- ğŸ“ Detailed error reporting per source
- ğŸ”„ Parallel execution for speed

**API:** `/api/memories/search`

**Example:**
```typescript
const response = await searchMemories({
  query: "latest AI developments",
  sources: ["indexed", "web"],
  answer: true,
  userId: user.id
});

// Check for errors (graceful degradation)
if (response.errors.length > 0) {
  console.warn("Some sources failed:", response.errors);
}

// Still get results from successful sources!
console.log(response.answer);
```

---

## âœ… 4. Collections & Tagging System

**What it does:** Organize your memories into collections like folders

**Features:**
- ğŸ“ Create collections (e.g., "poems", "research", "work-notes")
- ğŸ·ï¸ Tag memories for better organization
- ğŸ” Filter searches by collection
- ğŸ“Š Rich metadata support (JSONB)
- âš¡ Auto-create collections on first use

**API:** `/api/collections`

**Example:**
```typescript
// Create collection
await createCollection({
  name: "research-papers",
  description: "My AI research",
  userId: user.id
});

// Add memory to collection
await addMemory({
  text: "Important research findings...",
  collection: "research-papers",
  tags: ["AI", "ML", "NLP"],
  userId: user.id
});

// Search within collection
const results = await searchMemories({
  query: "transformer architecture",
  sources: ["vault"],
  options: {
    vault: { collection: "research-papers" }
  },
  userId: user.id
});
```

---

## âœ… 5. Document Processing Pipeline

**What it does:** Upload documents and automatically chunk, embed, and index them

**Features:**
- ğŸ“¤ Upload PDF, TXT, MD, or any text document
- âœ‚ï¸ Smart chunking with configurable overlap
- ğŸ”¢ Batch embedding generation
- ğŸ“Š Status tracking (uploading â†’ processing â†’ ready)
- ğŸ”„ Background processing via Supabase Edge Functions
- ğŸ” Automatic indexing for search

**API:** `/api/documents/upload`

**Example:**
```typescript
const result = await uploadDocument(
  file,
  user.id,
  "research-papers" // optional collection
);

console.log(`Document ${result.documentId} is ${result.status}`);

// Document is automatically:
// 1. Uploaded to storage
// 2. Split into chunks  
// 3. Embeddings generated
// 4. Indexed for search
```

---

## ğŸ”¥ NEW API Endpoints

### Memory Management
- `POST /api/memories/add` - Add memories with auto-embedding
- `POST /api/memories/search` - Multi-source search with error handling

### Search
- `POST /api/search/indexed` - Vector similarity search
- `POST /api/search/unified` - Hybrid indexed + live search

### AI Answers
- `POST /api/answer` - Multi-model answer generation
- `GET /api/answer` - List available models

### Collections
- `GET /api/collections` - List collections
- `POST /api/collections` - Create collection
- `PATCH /api/collections` - Update collection
- `DELETE /api/collections` - Delete collection

### Documents
- `POST /api/documents/upload` - Upload & process documents

### Utilities
- `POST /api/embeddings/generate` - Generate embeddings

---

## ğŸ“š Documentation

### Guides
1. **`MEMORY_SYSTEM_GUIDE.md`** - Complete API reference & architecture
2. **`MEMORY_EXAMPLES.md`** - Usage examples & patterns
3. **`SETUP_MEMORY_SYSTEM.md`** - Setup instructions & troubleshooting

### SDK
- **`frontend/lib/memory-client.ts`** - TypeScript SDK client

---

## ğŸ¯ Real-World Use Cases

### 1. Personal Knowledge Assistant
```typescript
// Add notes
await addMemory({
  text: "Meeting notes: Discussed Q4 strategy...",
  collection: "work-notes",
  tags: ["meetings", "Q4"],
  userId: user.id
});

// Search later with AI answer
const response = await searchMemories({
  query: "what did we discuss about Q4?",
  sources: ["vault"],
  answer: true,
  userId: user.id
});
```

### 2. Research Assistant
```typescript
// Upload research papers
await uploadDocument(pdfFile, user.id, "research");

// Query with web context
const response = await unifiedSearch({
  query: "recent advances in transformers",
  sources: ["indexed", "web"],
  mergeStrategy: "hybrid",
  answer: true,
  userId: user.id
});
```

### 3. Code Documentation
```typescript
// Add code snippets
await addMemory({
  text: `function authenticate(token) { ... }`,
  collection: "code-snippets",
  tags: ["auth", "jwt"],
  userId: user.id
});

// Search with AI explanation
const response = await searchMemories({
  query: "how do I verify JWT tokens?",
  sources: ["vault"],
  answer: true,
  answerModel: "gpt-4o", // Good for code
  userId: user.id
});
```

---

## ğŸ—ï¸ Architecture

```
Client App
    â†“
Next.js API Routes
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase      â”‚    OpenAI       â”‚   Anthropic      â”‚
â”‚   (PostgreSQL   â”‚   (Embeddings)  â”‚   (Claude)       â”‚
â”‚   + pgvector)   â”‚                 â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Database Tables:**
- `collections` - Memory collections
- `memories` - Memories with vector embeddings
- `documents` - Uploaded documents
- `document_chunks` - Document chunks
- `query_history` - Analytics

**Edge Functions:**
- `ingest-document` - Background document processing

---

## ğŸ”§ Integration with Existing Stack

### GEPA Integration
```typescript
// Use memory search in GEPA optimization
const memoryResults = await searchMemories({
  query: userQuery,
  sources: ["indexed", "web"],
  userId: user.id
});

// Feed results to GEPA
const gepaResponse = await fetch('/api/gepa/optimize', {
  method: 'POST',
  body: JSON.stringify({
    prompt: basePrompt,
    context: memoryResults.documents
  })
});
```

### AX Framework
```typescript
import { AxAI } from '@ax-llm/ax';

// Retrieve memories first
const memories = await searchMemories(query);

// Use AX with memory context
const result = await ai.chat([
  { role: 'system', content: buildContextFromMemories(memories) },
  { role: 'user', content: query }
]);
```

---

## ğŸ“Š Performance

- **Embedding Generation:** ~100ms
- **Vector Search:** ~50-200ms (10k vectors)
- **Document Processing:** ~1-5s per doc
- **Multi-Source Search:** ~500-2000ms (parallel)
- **Answer Generation:**
  - Haiku: ~500ms
  - Sonnet: ~1500ms
  - O1-mini: ~3000ms

---

## ğŸ’° Cost Estimates

### OpenAI Embeddings
- $0.02 / 1M tokens (~$0.00002 per embedding)
- 10,000 documents @ 500 tokens = **$0.10**

### Claude API
- Haiku: $0.25 / 1M input, $1.25 / 1M output
- 1,000 queries = **~$0.50**

### Total Monthly Cost
- **Small app** (1k queries): ~$1-2
- **Medium app** (10k queries): ~$10-20
- **Large app** (100k queries): ~$100-200

---

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
cd frontend
npm install openai @anthropic-ai/sdk
```

### 2. Run Migration
```bash
supabase db push
```

### 3. Set Environment Variables
```bash
OPENAI_API_KEY=your-key
ANTHROPIC_API_KEY=your-key
SUPABASE_SERVICE_ROLE_KEY=your-key
```

### 4. Deploy Edge Function
```bash
supabase functions deploy ingest-document
```

### 5. Use the SDK
```typescript
import { addMemory, searchMemories } from '@/lib/memory-client';

await addMemory({
  text: "Your content",
  collection: "my-collection",
  userId: user.id
});

const results = await searchMemories({
  query: "search query",
  sources: ["vault"],
  answer: true,
  userId: user.id
});
```

---

## ğŸ¯ What Makes This Different from Hyperspell?

| Feature | Hyperspell | Your System |
|---------|-----------|------------|
| **Vector DB** | Proprietary | PostgreSQL + pgvector |
| **Embeddings** | Custom | OpenAI text-embedding-3-small |
| **Models** | Llama, Gemma | Claude, GPT-4o, O1 |
| **Integration** | Standalone | Integrated with GEPA/AX |
| **Hosting** | SaaS | Self-hosted |
| **Cost** | Subscription | Pay-per-use |
| **Customization** | Limited | Full control |

---

## ğŸ“– Next Steps

1. **Read the guides:**
   - `MEMORY_SYSTEM_GUIDE.md` - Full API reference
   - `MEMORY_EXAMPLES.md` - Usage examples
   - `SETUP_MEMORY_SYSTEM.md` - Setup instructions

2. **Try the SDK:**
   - Import from `@/lib/memory-client`
   - See examples in `MEMORY_EXAMPLES.md`

3. **Build features:**
   - Personal knowledge assistant
   - Research assistant
   - Code documentation search
   - Customer support KB

---

**ğŸ‰ You now have a complete Hyperspell-like memory system with:**
- âœ… Vector search & indexed memories
- âœ… Multi-model answer routing
- âœ… Multi-source search with error handling
- âœ… Collections & tagging
- âœ… Document processing pipeline

**Built with:** Next.js, Supabase, OpenAI, Anthropic, and your existing GEPA/AX stack ğŸš€
